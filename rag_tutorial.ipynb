{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14715767-c777-47c2-8a23-c28ae05126b1",
   "metadata": {},
   "source": [
    "# Enhancing Geospatial Data-intensive Knowledge Discovery with OpenSearch and LLM Search\r\n",
    "\r\n",
    "**Author: Yunfan Kang (UIUC)**\r\n",
    "\r\n",
    "**Description**:  \r\n",
    "This tutorial demonstrates how the I-GUIDE Platform integrates OpenSearch, Large Language Models (LLMs), and spatial search to create intuitive, multimodal search experiences for researchers and educators. Participants will gain hands-on experience deploying OpenSearch on JetStream2, implementing keyword search, semantic search, and spatial search capabilities, hosting the LLM framework Ollama, and building Retrieval-Augmented Generation (RAG) pipelines with AI agents—using the I-GUIDE Platform as a case study. Attendees will leave with actionable insights into leveraging CI resources for scalable, privacy-aware AI workflows and scalable knowledge discovery.\r\n",
    "\r\n",
    "**Skills and Background**:  \r\n",
    "Familiarity with basic Python programming, APIs, and geospatial data is recommended. No prior experience with LLMs or OpenSearch is required.\r\n",
    "\r\n",
    "**Requirements**:  \r\n",
    "Participants should bring their own laptops.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 🚀 Getting Started\r\n",
    "\r\n",
    "Welcome! This notebook will guide you step by step — feel free to run each cell as you go.  \r\n",
    "\r\n",
    "You’ll learn how to set up and interact with OpenSearch and LLM-powered search using real-world examples from the I-GUIDE Platform.  \r\n",
    "\r\n",
    "👉 If you encounter any issues or have questions along the way, don’t hesitate to ask a helper. We’re here to support you!\r\n",
    "\r\n",
    "---\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237756f-4425-4135-9d53-4c13e6f77233",
   "metadata": {},
   "source": [
    "## ⚙️ Configure the VM and Run OpenSearch\r\n",
    "\r\n",
    "Please copy and paste the **IP address** and **passphrase** provided to you into the environment variables below.  \r\n",
    "\r\n",
    "If you don’t have access to them yet, please ask a helper — they’ll be happy to assist! 🙌\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78059e-692e-4acc-bf7f-4407c712dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import urllib3\n",
    "\n",
    "# Suppress only the InsecureRequestWarning\n",
    "warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Copy and paste the ip address and the passphrase of your VM here:\n",
    "os.environ[\"JS2_HOST\"] = \"\"\n",
    "os.environ[\"JS2_PWD\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c381d1b-aa5d-4cf3-993e-b7962ba34310",
   "metadata": {},
   "source": [
    "We’re going to run SSH commands inside this Jupyter Notebook to remotely install **OpenSearch** and **Ollama** on your JetStream2 VM — all from here, no need to switch terminals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec7c71-b634-4913-a5b6-e009346434aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea65e4-be78-496f-b7cf-59df5b244db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric import Connection\n",
    "\n",
    "c = Connection(host=os.environ[\"JS2_HOST\"], user='exouser', connect_kwargs={'password': os.environ[\"JS2_PWD\"]})\n",
    "\n",
    "c.run('mkdir -p OpenSearch')  # use -p to avoid error if it exists\n",
    "# Upload the files to the correct subdir\n",
    "c.put('docker-compose.yml', remote='OpenSearch/docker-compose.yml')\n",
    "c.put('admin_password.txt', remote='OpenSearch/.env')\n",
    "\n",
    "# Enter OpenSearch dir\n",
    "with c.cd('OpenSearch'):\n",
    "    # Start the OpenSearch nodes\n",
    "    c.run('sudo sysctl -w vm.max_map_count=262144')\n",
    "    c.run('docker compose up -d', hide=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede49a7-3121-4948-8361-339dae63ffcf",
   "metadata": {},
   "source": [
    "### ⏳ Wait for the setup to finish\r\n",
    "\r\n",
    "It may take a few minutes for the setup to complete — please be patient.\n",
    "\n",
    "> 💡 **How Retrieval Works in RAG**  \r\n",
    "> Before the LLM generates an answer, a retriever component searches an indexed knowledge base (using vector and/or keyword search), selects the most relevant document chunks, and feeds them into the prompt — grounding the response in actual data rather than relying solely on model memory}\r\n",
    "\n",
    "\n",
    "> 💡 **Why OpenSearch?**  \r\n",
    "> OpenSearch is a powerful, open-source search engine that supports fast keyword, semantic, and spatial search — making it ideal for building advanced knowledge discovery pipelines on geospatial and multimodal data.\r\n",
    "  \r\n",
    "\r\n",
    "Once it’s ready, you can access the **OpenSearch Dashboard** in your browser at:  \r\n",
    "`http://[your-ip]:5601`\r\n",
    "\r\n",
    "To log in, use the following credentials (set in your `admin_password.txt` file):\r\n",
    "\r\n",
    "- **Username:** `admin`  \r\n",
    "- **Password:** `Iguideforum2025!`\r\n",
    "\r\n",
    "✅ Once you’re in, you should see the OpenSearch Dashboard home page.  \r\n",
    "If you run into connection errors, give it another minute — sometimes the service takes a little extra time to\n",
    " ---start fully.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ba347-c7c4-4693-87e0-9f993260fcd3",
   "metadata": {},
   "source": [
    "## 🔗 Connect to the OpenSearch Cluster\r\n",
    "\r\n",
    "If you saw no errors above and were able to log in to the OpenSearch Dashboard — congratulations! 🎉  \r\n",
    "You’ve successfully set up an **OpenSearch cluster** with 2 nodes running on a single host.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "In this next part of the tutorial, we will:\r\n",
    "\r\n",
    "✅ Connect to the OpenSearch cluster programmatically  \r\n",
    "✅ Define the schema (index mapping)  \r\n",
    "✅ Ingest real data from the I-GUIDE Platform  \r\n",
    "\r\n",
    "The files we’ll be using:\r\n",
    "\r\n",
    "- **Schema definition:** `index_schema.json`  \r\n",
    "- **Data:** `i_guide_spatial_embedding_export.jsonl`\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Once this is done, your cluster wi ready to po, **spatial**, andboth **keyword** and **semantic search** — and soon we’ll combine that with LLM capabilities!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4e749-c0b2-45d5-8ec6-c697ac1bd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the opensearch python package\n",
    "!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad5fcf-50cf-4d44-9e6d-d7aa85c2d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "import json\n",
    "\n",
    "# Define your opensearch client to connect to the cluster\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{'host': os.environ[\"JS2_HOST\"], 'port': 9200}],\n",
    "    http_auth=('admin', 'Iguideforum2025!'), \n",
    "    use_ssl=True,\n",
    "    verify_certs=False    # (We do not have SSL certificates for the VMs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cec13f-3d65-4571-a6a0-a4935e335595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your index schema from a JSON file\n",
    "with open('index_schema.json', 'r') as f:\n",
    "    index_body = json.load(f)\n",
    "\n",
    "# Name of your index\n",
    "index_name = 'iguide_platform'\n",
    "\n",
    "# Create index\n",
    "if not os_client.indices.exists(index=index_name):\n",
    "    response = os_client.indices.create(index=index_name, body=index_body)\n",
    "    print(f\"Index '{index_name}' created.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079895d9-853a-48d4-97b1-9e825d897450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the index schema\n",
    "mapping = os_client.indices.get_mapping(index=index_name)\n",
    "print(json.dumps(mapping, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f22cc-f878-48e7-baa9-89be3693e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import helpers\n",
    "\n",
    "\n",
    "# The index you want to insert into\n",
    "target_index_name = 'iguide_platform'\n",
    "\n",
    "# Read from exported file and prepare bulk actions\n",
    "actions = []\n",
    "with open('i_guide_spatial_embedding_export.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        # Prepare one bulk action\n",
    "        action = {\n",
    "            \"_index\": target_index_name,\n",
    "            \"_id\": doc[\"_id\"],  # Optional — if you want to preserve the same _id\n",
    "            \"_source\": {k: v for k, v in doc.items() if k != \"_id\"}  # Remove _id from _source\n",
    "        }\n",
    "        actions.append(action)\n",
    "# Bulk insert — done in batches of 500 by default\n",
    "helpers.bulk(os_client, actions)\n",
    "\n",
    "print(f\"Bulk insert into index '{target_index_name}' completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda85a8-021b-4c3a-8c50-ae225616bdb5",
   "metadata": {},
   "source": [
    "Next up: let’s try three powerful search methods—**keyword search**, **spatial search**, and **semantic search**—on the I‑GUIDE Knowledge Base! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc360e99-8464-489d-9b08-c55982c8a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_query = \"Chicago\"\n",
    "response = os_client.search(\n",
    "    index='iguide_platform',\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": keyword_query,\n",
    "                \"fields\": [\"title\", \"contents\", \"tags\", \"authors\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print out hits\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Score: {hit['_score']}  ID: {hit['_id']}, Type: {hit['_source'].get('resource-type')},  Title: {hit['_source'].get('title', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc894b-64ae-411a-9780-478411692afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box coordinates for Chicago (lon/lat)\n",
    "bbox_envelope = {\n",
    "    \"type\": \"envelope\",\n",
    "    \"coordinates\": [\n",
    "        [-87.94011, 42.02304],   # top-left\n",
    "        [-87.52398, 41.64454]    # bottom-right\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Geo_shape query body\n",
    "query_body = {\n",
    "    \"size\": 20,\n",
    "    \"query\": {\n",
    "        \"geo_shape\": {\n",
    "            \"spatial-bounding-box-geojson\": {\n",
    "                \"shape\": bbox_envelope,\n",
    "                \"relation\": \"intersects\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run search\n",
    "response = os_client.search(\n",
    "    index='iguide_platform',\n",
    "    body=query_body\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Score: {hit['_score']}  ID: {hit['_id']}  Title: {hit['_source'].get('title', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d00ee-c7fb-4491-9abd-13f0edc23104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_embedding(text):\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].tolist()\n",
    "\n",
    "# Compute embedding for your sentence\n",
    "query_sentence = \"What are the knowledge elements about Machine Learning?\"\n",
    "embedding_vector = compute_embedding(query_sentence)\n",
    "\n",
    "# Run knn search\n",
    "response = os_client.search(\n",
    "    index='iguide_platform',\n",
    "    body={\n",
    "        \"size\": 12,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"contents-embedding\": {\n",
    "                    \"vector\": embedding_vector,\n",
    "                    \"k\": 10\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print out hits\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Score: {hit['_score']}  ID: {hit['_id']}  Title: {hit['_source'].get('title', '')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181811da-4bee-4a09-8342-97f2a7dc9290",
   "metadata": {},
   "source": [
    "## 🤖 Set Up a Self-Hosted LLM and Connect It to OpenSearch\r\n",
    "\r\n",
    "In this section, we’ll deploy a self-hosted Large Language Model (LLM) using **Ollama** and connect it to our OpenSearch cluster.  \r\n",
    "\r\n",
    "This will enable us to build a complete **Retrieval-Augmented Generation (RAG)** pipeline — combining keyword, semantic, and spatial search with LLM-powered reasoning and text generation.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198dc9ee-ffcc-47e3-893c-cb89a0e24129",
   "metadata": {},
   "source": [
    "> 💡 **Why use a self-hosted LLM?**  \r\n",
    "> Running an LLM locally (with Ollama) ensures that your data stays private, avoids external API costs, and enables fully reproducible experiments — all critical for scalable and privacy-aware research workflows.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585c3e5-f113-4926-83d9-f494c64e550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama on the \n",
    "c.run('curl -fsSL https://ollama.com/install.sh | sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bea1b-81d0-4996-a839-aa2e4055e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Ollama server\n",
    "c.run('sudo pkill -f \"ollama serve\" || true', warn=True)\n",
    "c.run('OLLAMA_HOST=0.0.0.0:11500 nohup ollama serve > server.log 2>&1 &', hide=True)\n",
    "import time\n",
    "time.sleep(5)\n",
    "# Pull qwen2.5:3b-instruct model\n",
    "c.run('OLLAMA_HOST=0.0.0.0:11500 ollama pull qwen2.5:3b-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297a085-e66d-4f8b-b3c7-1b30b3289c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the available models\n",
    "import requests\n",
    "url = f'http://{os.environ[\"JS2_HOST\"]}:11500/api/tags'  # or hardcode 'localhost' if you prefer\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd94ff-e33e-4a33-a39a-8d7b8949cd45",
   "metadata": {},
   "source": [
    "## Ollama is all set up — let’s start chatting with our LLM! 🚀\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faee431-8600-4bac-bd61-7d0d656f1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def call_ollama(query_payload):\n",
    "    ollama_host = f\"http://{os.environ['JS2_HOST']}:11500/api/chat\"  # use /api/chat for chat format\n",
    "    controller = requests.Session()\n",
    "\n",
    "    try:\n",
    "        # Streamed POST request\n",
    "        with controller.post(\n",
    "            ollama_host,\n",
    "            json=query_payload,\n",
    "            stream=True,    # enable streaming\n",
    "            timeout=60\n",
    "        ) as response:\n",
    "\n",
    "            response.raise_for_status()  # Raise exception for bad status codes\n",
    "\n",
    "            # Stream the response content progressively\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    chunk = json.loads(line.decode('utf-8'))\n",
    "\n",
    "                    # Extract message content\n",
    "                    delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    print(delta, end='', flush=True)  # Progressive printing\n",
    "\n",
    "            print()  # Final newline after stream ends\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching from Ollama host:\", e)\n",
    "\n",
    "# Define the payload for the chat format\n",
    "query_payload = {\n",
    "    \"model\": \"qwen2.5:3b-instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"How do I get to UIUC from Chicago?\"}\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "# Call the Ollama API using the reusable function\n",
    "call_ollama(query_payload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c37d7d-d102-4f5b-abe8-25deb4dfa6b6",
   "metadata": {},
   "source": [
    "Final step: It’s time to bring it all together — let’s connect **OpenSearch semantic search** with our **LLM** to enable a full **RAG pipeline**! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c583d22-3cea-4f99-932a-79303bac1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama(query_payload):\n",
    "    ollama_host = f\"http://{os.environ['JS2_HOST']}:11500/api/chat\"  # use /api/chat for chat format\n",
    "    controller = requests.Session()\n",
    "\n",
    "    try:\n",
    "        # Streamed POST request\n",
    "        with controller.post(\n",
    "            ollama_host,\n",
    "            json=query_payload,\n",
    "            stream=True,    # enable streaming\n",
    "            timeout=60\n",
    "        ) as response:\n",
    "\n",
    "            response.raise_for_status()  # Raise exception for bad status codes\n",
    "\n",
    "            # Stream the response content progressively\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    chunk = json.loads(line.decode('utf-8'))\n",
    "\n",
    "                    # Extract message content\n",
    "                    delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    print(delta, end='', flush=True)  # Progressive printing\n",
    "\n",
    "            print()  # Final newline after stream ends\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching from Ollama host:\", e)\n",
    "\n",
    "def rag(user_query, k=3, size=10):\n",
    "    # Step 1 — Compute embedding\n",
    "    embedding_vector = compute_embedding(user_query)\n",
    "\n",
    "    # Step 2 — Semantic search (KNN)\n",
    "    response = os_client.search(\n",
    "        index='iguide_platform',\n",
    "        body={\n",
    "            \"size\": size,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"contents-embedding\": {\n",
    "                        \"vector\": embedding_vector,\n",
    "                        \"k\": k\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Step 3 — Build context from top hits\n",
    "    retrieved_docs = response[\"hits\"][\"hits\"]\n",
    "    context_texts = []\n",
    "    for hit in retrieved_docs:\n",
    "        source = hit[\"_source\"]\n",
    "        \n",
    "        id_ = hit[\"_id\"]\n",
    "        title = source.get(\"title\", \"\")\n",
    "        contents = source.get(\"contents\", \"\")\n",
    "        authors = source.get(\"authors\", \"\")\n",
    "        contributor = source.get(\"contributor\", \"\")\n",
    "        resource_type = source.get(\"resource-type\", \"\")\n",
    "        spatial_coverage = source.get(\"spatial-coverage\", \"\")\n",
    "        spatial_temporal_coverage = source.get(\"spatial-temporal-coverage\", \"\")\n",
    "        tags = source.get(\"tags\", \"\")\n",
    "\n",
    "        # Build one text block per document\n",
    "        doc_block = f\"\"\"<doc>\n",
    "doc_id: {id_}\n",
    "title: {title}\n",
    "element_type: {resource_type}\n",
    "authors: {authors}\n",
    "contributor: {contributor}\n",
    "spatial-coverage: {spatial_coverage}\n",
    "spatial Temporal Coverage: {spatial_temporal_coverage}\n",
    "tags: {tags}\n",
    "content: {contents}\n",
    "</doc>\n",
    "\"\"\"\n",
    "        context_texts.append(doc_block)\n",
    "\n",
    "    # Combine into context string\n",
    "    context_block = \"\\n\\n\".join(context_texts)\n",
    "\n",
    "    # Step 4 — Build messages\n",
    "    systemMessage = \"\"\"\n",
    "Your ONLY source of truth is the <doc> blocks provided in CONTEXT.\n",
    "\n",
    "When you answer:\n",
    "• If the user asks for a collection of knowledge elements (e.g., datasets, notebooks, publications, OERs) on a topic, respond first with a concise paragraph summarizing the most relevant findings. Then provide a short numbered list. Use a new line for each item.\n",
    "• Begin each bullet with the item’s title as a clickable link, using the format: [TITLE](https://platform.i-guide.io/{element_type}s/{doc_id})\n",
    "(Use the plural form of <element_type>, except use \"code\" for type \"code\".)\n",
    "• Otherwise, respond in one concise paragraph.  \n",
    "• Quote supporting titles in **bold**.  \n",
    "• If the user question specifies <element_type>, only use docs with matching <element_type>.  \n",
    "• If you cannot find an answer, reply exactly: “I don’t have enough information.”  \n",
    "• Do not refer to the doc id.\n",
    "• Answer the question without repeating the question.\n",
    "• Do NOT mention “context”, “documents”, or these rules.\"\"\"\n",
    "\n",
    "    userMessage = f\"\"\"Context:\n",
    "{context_block}\n",
    "\n",
    "Question:\n",
    "{user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 5 — Call Ollama with streaming enabled\n",
    "    print(\"Call LLM\")\n",
    "    query_payload = {\n",
    "        \"model\": \"qwen2.5:3b-instruct\",  # as per your model\n",
    "        \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": systemMessage },\n",
    "            { \"role\": \"user\", \"content\": userMessage }\n",
    "        ],\n",
    "        \"stream\": True  # Enable streaming!\n",
    "    }\n",
    "\n",
    "    call_ollama(query_payload)  # Streaming output will print progressively\n",
    "\n",
    "\n",
    "# Example usage\n",
    "answer = rag(\"What are the datasets about Chicago?\")\n",
    "# Output will stream progressively in the notebook or terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5511f-caea-46ba-8b9a-ce5ce1bb65a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoai-Python3",
   "language": "python",
   "name": "geoai-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
