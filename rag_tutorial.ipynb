{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14715767-c777-47c2-8a23-c28ae05126b1",
   "metadata": {},
   "source": [
    "# Enhancing Geospatial Data-intensive Knowledge Discovery with OpenSearch and LLM Search\r\n",
    "\r\n",
    "**Author: Yunfan Kang (UIUC)**\r\n",
    "\r\n",
    "**Description**:  \r\n",
    "This tutorial demonstrates how the I-GUIDE Platform integrates OpenSearch, Large Language Models (LLMs), and spatial search to create intuitive, multimodal search experiences for researchers and educators. Participants will gain hands-on experience deploying OpenSearch on JetStream2, implementing keyword search, semantic search, and spatial search capabilities, hosting the LLM framework Ollama, and building Retrieval-Augmented Generation (RAG) pipelines with AI agents‚Äîusing the I-GUIDE Platform as a case study. Attendees will leave with actionable insights into leveraging CI resources for scalable, privacy-aware AI workflows and scalable knowledge discovery.\r\n",
    "\r\n",
    "**Skills and Background**:  \r\n",
    "Familiarity with basic Python programming, APIs, and geospatial data is recommended. No prior experience with LLMs or OpenSearch is required.\r\n",
    "\r\n",
    "**Requirements**:  \r\n",
    "Participants should bring their own laptops.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## üöÄ Getting Started\r\n",
    "\r\n",
    "Welcome! This notebook will guide you step by step ‚Äî feel free to run each cell as you go.  \r\n",
    "\r\n",
    "You‚Äôll learn how to set up and interact with OpenSearch and LLM-powered search using real-world examples from the I-GUIDE Platform.  \r\n",
    "\r\n",
    "üëâ If you encounter any issues or have questions along the way, don‚Äôt hesitate to ask a helper. We‚Äôre here to support you!\r\n",
    "\r\n",
    "---\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237756f-4425-4135-9d53-4c13e6f77233",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configure the VM and Run OpenSearch\r\n",
    "\r\n",
    "Please copy and paste the **IP address** and **passphrase** provided to you into the environment variables below.  \r\n",
    "\r\n",
    "If you don‚Äôt have access to them yet, please ask a helper ‚Äî they‚Äôll be happy to assist! üôå\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78059e-692e-4acc-bf7f-4407c712dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import urllib3\n",
    "\n",
    "# Suppress only the InsecureRequestWarning\n",
    "warnings.filterwarnings('ignore', category=urllib3.exceptions.InsecureRequestWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Copy and paste the ip address and the passphrase of your VM here:\n",
    "os.environ[\"JS2_HOST\"] = \"\"\n",
    "os.environ[\"JS2_PWD\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c381d1b-aa5d-4cf3-993e-b7962ba34310",
   "metadata": {},
   "source": [
    "We‚Äôre going to run SSH commands inside this Jupyter Notebook to remotely install **OpenSearch** and **Ollama** on your JetStream2 VM ‚Äî all from here, no need to switch terminals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec7c71-b634-4913-a5b6-e009346434aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea65e4-be78-496f-b7cf-59df5b244db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabric import Connection\n",
    "\n",
    "c = Connection(host=os.environ[\"JS2_HOST\"], user='exouser', connect_kwargs={'password': os.environ[\"JS2_PWD\"]})\n",
    "\n",
    "c.run('mkdir -p OpenSearch')  # use -p to avoid error if it exists\n",
    "# Upload the files to the correct subdir\n",
    "c.put('docker-compose.yml', remote='OpenSearch/docker-compose.yml')\n",
    "c.put('admin_password.txt', remote='OpenSearch/.env')\n",
    "\n",
    "# Enter OpenSearch dir\n",
    "with c.cd('OpenSearch'):\n",
    "    # Start the OpenSearch nodes\n",
    "    c.run('sudo sysctl -w vm.max_map_count=262144')\n",
    "    c.run('docker compose up -d', hide=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede49a7-3121-4948-8361-339dae63ffcf",
   "metadata": {},
   "source": [
    "### ‚è≥ Wait for the setup to finish\r\n",
    "\r\n",
    "It may take a few minutes for the setup to complete ‚Äî please be patient.\n",
    "\n",
    "> üí° **How Retrieval Works in RAG**  \r\n",
    "> Before the LLM generates an answer, a retriever component searches an indexed knowledge base (using vector and/or keyword search), selects the most relevant document chunks, and feeds them into the prompt ‚Äî grounding the response in actual data rather than relying solely on model memory}\r\n",
    "\n",
    "\n",
    "> üí° **Why OpenSearch?**  \r\n",
    "> OpenSearch is a powerful, open-source search engine that supports fast keyword, semantic, and spatial search ‚Äî making it ideal for building advanced knowledge discovery pipelines on geospatial and multimodal data.\r\n",
    "  \r\n",
    "\r\n",
    "Once it‚Äôs ready, you can access the **OpenSearch Dashboard** in your browser at:  \r\n",
    "`http://[your-ip]:5601`\r\n",
    "\r\n",
    "To log in, use the following credentials (set in your `admin_password.txt` file):\r\n",
    "\r\n",
    "- **Username:** `admin`  \r\n",
    "- **Password:** `Iguideforum2025!`\r\n",
    "\r\n",
    "‚úÖ Once you‚Äôre in, you should see the OpenSearch Dashboard home page.  \r\n",
    "If you run into connection errors, give it another minute ‚Äî sometimes the service takes a little extra time to\n",
    " ---start fully.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ba347-c7c4-4693-87e0-9f993260fcd3",
   "metadata": {},
   "source": [
    "## üîó Connect to the OpenSearch Cluster\r\n",
    "\r\n",
    "If you saw no errors above and were able to log in to the OpenSearch Dashboard ‚Äî congratulations! üéâ  \r\n",
    "You‚Äôve successfully set up an **OpenSearch cluster** with 2 nodes running on a single host.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "In this next part of the tutorial, we will:\r\n",
    "\r\n",
    "‚úÖ Connect to the OpenSearch cluster programmatically  \r\n",
    "‚úÖ Define the schema (index mapping)  \r\n",
    "‚úÖ Ingest real data from the I-GUIDE Platform  \r\n",
    "\r\n",
    "The files we‚Äôll be using:\r\n",
    "\r\n",
    "- **Schema definition:** `index_schema.json`  \r\n",
    "- **Data:** `i_guide_spatial_embedding_export.jsonl`\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Once this is done, your cluster wi ready to po, **spatial**, andboth **keyword** and **semantic search** ‚Äî and soon we‚Äôll combine that with LLM capabilities!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4e749-c0b2-45d5-8ec6-c697ac1bd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the opensearch python package\n",
    "!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad5fcf-50cf-4d44-9e6d-d7aa85c2d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "import json\n",
    "\n",
    "# Define your opensearch client to connect to the cluster\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{'host': os.environ[\"JS2_HOST\"], 'port': 9200}],\n",
    "    http_auth=('admin', 'Iguideforum2025!'), \n",
    "    use_ssl=True,\n",
    "    verify_certs=False    # (We do not have SSL certificates for the VMs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cec13f-3d65-4571-a6a0-a4935e335595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your index schema from a JSON file\n",
    "with open('index_schema.json', 'r') as f:\n",
    "    index_body = json.load(f)\n",
    "\n",
    "# Name of your index\n",
    "index_name = 'iguide_platform'\n",
    "\n",
    "# Create index\n",
    "if not os_client.indices.exists(index=index_name):\n",
    "    response = os_client.indices.create(index=index_name, body=index_body)\n",
    "    print(f\"Index '{index_name}' created.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079895d9-853a-48d4-97b1-9e825d897450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the index schema\n",
    "mapping = os_client.indices.get_mapping(index=index_name)\n",
    "print(json.dumps(mapping, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f22cc-f878-48e7-baa9-89be3693e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import helpers\n",
    "\n",
    "\n",
    "# The index you want to insert into\n",
    "target_index_name = 'iguide_platform'\n",
    "\n",
    "# Read from exported file and prepare bulk actions\n",
    "actions = []\n",
    "with open('i_guide_spatial_embedding_export.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        # Prepare one bulk action\n",
    "        action = {\n",
    "            \"_index\": target_index_name,\n",
    "            \"_id\": doc[\"_id\"],  # Optional ‚Äî if you want to preserve the same _id\n",
    "            \"_source\": {k: v for k, v in doc.items() if k != \"_id\"}  # Remove _id from _source\n",
    "        }\n",
    "        actions.append(action)\n",
    "# Bulk insert ‚Äî done in batches of 500 by default\n",
    "helpers.bulk(os_client, actions)\n",
    "\n",
    "print(f\"Bulk insert into index '{target_index_name}' completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda85a8-021b-4c3a-8c50-ae225616bdb5",
   "metadata": {},
   "source": [
    "Next up: let‚Äôs try three powerful search methods‚Äî**keyword search**, **spatial search**, and **semantic search**‚Äîon the I‚ÄëGUIDE Knowledge Base! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc360e99-8464-489d-9b08-c55982c8a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_query = \"Chicago\"\n",
    "response = os_client.search(\n",
    "    index='iguide_platform',\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": keyword_query,\n",
    "                \"fields\": [\"title\", \"contents\", \"tags\", \"authors\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print out hits\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Score: {hit['_score']}  ID: {hit['_id']}, Type: {hit['_source'].get('resource-type')},  Title: {hit['_source'].get('title', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc894b-64ae-411a-9780-478411692afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box coordinates for Chicago (lon/lat)\n",
    "bbox_envelope = {\n",
    "    \"type\": \"envelope\",\n",
    "    \"coordinates\": [\n",
    "        [-87.94011, 42.02304],   # top-left\n",
    "        [-87.52398, 41.64454]    # bottom-right\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Geo_shape query body\n",
    "query_body = {\n",
    "    \"size\": 20,\n",
    "    \"query\": {\n",
    "        \"geo_shape\": {\n",
    "            \"spatial-bounding-box-geojson\": {\n",
    "                \"shape\": bbox_envelope,\n",
    "                \"relation\": \"intersects\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run search\n",
    "response = os_client.search(\n",
    "    index='iguide_platform',\n",
    "    body=query_body\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Score: {hit['_score']}  ID: {hit['_id']}  Title: {hit['_source'].get('title', '')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d00ee-c7fb-4491-9abd-13f0edc23104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_embedding(text):\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].tolist()\n",
    "\n",
    "# Compute embedding for your sentence\n",
    "query_sentence = \"What are the knowledge elements about Machine Learning?\"\n",
    "embedding_vector = compute_embedding(query_sentence)\n",
    "\n",
    "# Run knn search\n",
    "response = os_client.search(\n",
    "    index='iguide_platform',\n",
    "    body={\n",
    "        \"size\": 12,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"contents-embedding\": {\n",
    "                    \"vector\": embedding_vector,\n",
    "                    \"k\": 10\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print out hits\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Score: {hit['_score']}  ID: {hit['_id']}  Title: {hit['_source'].get('title', '')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181811da-4bee-4a09-8342-97f2a7dc9290",
   "metadata": {},
   "source": [
    "## ü§ñ Set Up a Self-Hosted LLM and Connect It to OpenSearch\r\n",
    "\r\n",
    "In this section, we‚Äôll deploy a self-hosted Large Language Model (LLM) using **Ollama** and connect it to our OpenSearch cluster.  \r\n",
    "\r\n",
    "This will enable us to build a complete **Retrieval-Augmented Generation (RAG)** pipeline ‚Äî combining keyword, semantic, and spatial search with LLM-powered reasoning and text generation.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198dc9ee-ffcc-47e3-893c-cb89a0e24129",
   "metadata": {},
   "source": [
    "> üí° **Why use a self-hosted LLM?**  \r\n",
    "> Running an LLM locally (with Ollama) ensures that your data stays private, avoids external API costs, and enables fully reproducible experiments ‚Äî all critical for scalable and privacy-aware research workflows.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585c3e5-f113-4926-83d9-f494c64e550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama on the \n",
    "c.run('curl -fsSL https://ollama.com/install.sh | sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bea1b-81d0-4996-a839-aa2e4055e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Ollama server\n",
    "c.run('sudo pkill -f \"ollama serve\" || true', warn=True)\n",
    "c.run('OLLAMA_HOST=0.0.0.0:11500 nohup ollama serve > server.log 2>&1 &', hide=True)\n",
    "import time\n",
    "time.sleep(5)\n",
    "# Pull qwen2.5:3b-instruct model\n",
    "c.run('OLLAMA_HOST=0.0.0.0:11500 ollama pull qwen2.5:3b-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297a085-e66d-4f8b-b3c7-1b30b3289c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the available models\n",
    "import requests\n",
    "url = f'http://{os.environ[\"JS2_HOST\"]}:11500/api/tags'  # or hardcode 'localhost' if you prefer\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd94ff-e33e-4a33-a39a-8d7b8949cd45",
   "metadata": {},
   "source": [
    "## Ollama is all set up ‚Äî let‚Äôs start chatting with our LLM! üöÄ\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faee431-8600-4bac-bd61-7d0d656f1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def call_ollama(query_payload):\n",
    "    ollama_host = f\"http://{os.environ['JS2_HOST']}:11500/api/chat\"  # use /api/chat for chat format\n",
    "    controller = requests.Session()\n",
    "\n",
    "    try:\n",
    "        # Streamed POST request\n",
    "        with controller.post(\n",
    "            ollama_host,\n",
    "            json=query_payload,\n",
    "            stream=True,    # enable streaming\n",
    "            timeout=60\n",
    "        ) as response:\n",
    "\n",
    "            response.raise_for_status()  # Raise exception for bad status codes\n",
    "\n",
    "            # Stream the response content progressively\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    chunk = json.loads(line.decode('utf-8'))\n",
    "\n",
    "                    # Extract message content\n",
    "                    delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    print(delta, end='', flush=True)  # Progressive printing\n",
    "\n",
    "            print()  # Final newline after stream ends\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching from Ollama host:\", e)\n",
    "\n",
    "# Define the payload for the chat format\n",
    "query_payload = {\n",
    "    \"model\": \"qwen2.5:3b-instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"How do I get to UIUC from Chicago?\"}\n",
    "    ],\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "# Call the Ollama API using the reusable function\n",
    "call_ollama(query_payload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c37d7d-d102-4f5b-abe8-25deb4dfa6b6",
   "metadata": {},
   "source": [
    "Final step: It‚Äôs time to bring it all together ‚Äî let‚Äôs connect **OpenSearch semantic search** with our **LLM** to enable a full **RAG pipeline**! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c583d22-3cea-4f99-932a-79303bac1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama(query_payload):\n",
    "    ollama_host = f\"http://{os.environ['JS2_HOST']}:11500/api/chat\"  # use /api/chat for chat format\n",
    "    controller = requests.Session()\n",
    "\n",
    "    try:\n",
    "        # Streamed POST request\n",
    "        with controller.post(\n",
    "            ollama_host,\n",
    "            json=query_payload,\n",
    "            stream=True,    # enable streaming\n",
    "            timeout=60\n",
    "        ) as response:\n",
    "\n",
    "            response.raise_for_status()  # Raise exception for bad status codes\n",
    "\n",
    "            # Stream the response content progressively\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    chunk = json.loads(line.decode('utf-8'))\n",
    "\n",
    "                    # Extract message content\n",
    "                    delta = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    print(delta, end='', flush=True)  # Progressive printing\n",
    "\n",
    "            print()  # Final newline after stream ends\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching from Ollama host:\", e)\n",
    "\n",
    "def rag(user_query, k=3, size=10):\n",
    "    # Step 1 ‚Äî Compute embedding\n",
    "    embedding_vector = compute_embedding(user_query)\n",
    "\n",
    "    # Step 2 ‚Äî Semantic search (KNN)\n",
    "    response = os_client.search(\n",
    "        index='iguide_platform',\n",
    "        body={\n",
    "            \"size\": size,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"contents-embedding\": {\n",
    "                        \"vector\": embedding_vector,\n",
    "                        \"k\": k\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Step 3 ‚Äî Build context from top hits\n",
    "    retrieved_docs = response[\"hits\"][\"hits\"]\n",
    "    context_texts = []\n",
    "    for hit in retrieved_docs:\n",
    "        source = hit[\"_source\"]\n",
    "        \n",
    "        id_ = hit[\"_id\"]\n",
    "        title = source.get(\"title\", \"\")\n",
    "        contents = source.get(\"contents\", \"\")\n",
    "        authors = source.get(\"authors\", \"\")\n",
    "        contributor = source.get(\"contributor\", \"\")\n",
    "        resource_type = source.get(\"resource-type\", \"\")\n",
    "        spatial_coverage = source.get(\"spatial-coverage\", \"\")\n",
    "        spatial_temporal_coverage = source.get(\"spatial-temporal-coverage\", \"\")\n",
    "        tags = source.get(\"tags\", \"\")\n",
    "\n",
    "        # Build one text block per document\n",
    "        doc_block = f\"\"\"<doc>\n",
    "doc_id: {id_}\n",
    "title: {title}\n",
    "element_type: {resource_type}\n",
    "authors: {authors}\n",
    "contributor: {contributor}\n",
    "spatial-coverage: {spatial_coverage}\n",
    "spatial Temporal Coverage: {spatial_temporal_coverage}\n",
    "tags: {tags}\n",
    "content: {contents}\n",
    "</doc>\n",
    "\"\"\"\n",
    "        context_texts.append(doc_block)\n",
    "\n",
    "    # Combine into context string\n",
    "    context_block = \"\\n\\n\".join(context_texts)\n",
    "\n",
    "    # Step 4 ‚Äî Build messages\n",
    "    systemMessage = \"\"\"\n",
    "Your ONLY source of truth is the <doc> blocks provided in CONTEXT.\n",
    "\n",
    "When you answer:\n",
    "‚Ä¢ If the user asks for a collection of knowledge elements (e.g., datasets, notebooks, publications, OERs) on a topic, respond first with a concise paragraph summarizing the most relevant findings. Then provide a short numbered list. Use a new line for each item.\n",
    "‚Ä¢ Begin each bullet with the item‚Äôs title as a clickable link, using the format: [TITLE](https://platform.i-guide.io/{element_type}s/{doc_id})\n",
    "(Use the plural form of <element_type>, except use \"code\" for type \"code\".)\n",
    "‚Ä¢ Otherwise, respond in one concise paragraph.  \n",
    "‚Ä¢ Quote supporting titles in **bold**.  \n",
    "‚Ä¢ If the user question specifies <element_type>, only use docs with matching <element_type>.  \n",
    "‚Ä¢ If you cannot find an answer, reply exactly: ‚ÄúI don‚Äôt have enough information.‚Äù  \n",
    "‚Ä¢ Do not refer to the doc id.\n",
    "‚Ä¢ Answer the question without repeating the question.\n",
    "‚Ä¢ Do NOT mention ‚Äúcontext‚Äù, ‚Äúdocuments‚Äù, or these rules.\"\"\"\n",
    "\n",
    "    userMessage = f\"\"\"Context:\n",
    "{context_block}\n",
    "\n",
    "Question:\n",
    "{user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 5 ‚Äî Call Ollama with streaming enabled\n",
    "    print(\"Call LLM\")\n",
    "    query_payload = {\n",
    "        \"model\": \"qwen2.5:3b-instruct\",  # as per your model\n",
    "        \"messages\": [\n",
    "            { \"role\": \"system\", \"content\": systemMessage },\n",
    "            { \"role\": \"user\", \"content\": userMessage }\n",
    "        ],\n",
    "        \"stream\": True  # Enable streaming!\n",
    "    }\n",
    "\n",
    "    call_ollama(query_payload)  # Streaming output will print progressively\n",
    "\n",
    "\n",
    "# Example usage\n",
    "answer = rag(\"What are the datasets about Chicago?\")\n",
    "# Output will stream progressively in the notebook or terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5511f-caea-46ba-8b9a-ce5ce1bb65a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoai-Python3",
   "language": "python",
   "name": "geoai-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
